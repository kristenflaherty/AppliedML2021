{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) how many training and test data points are there?\n",
    "\n",
    "**There are 7613 test data points, and 3263 test data points.**\n",
    "\n",
    "(2) what percentage of the training tweets are of real disasters, and what percentage is not? Note that the meaning of each column is explained in the data description on Kaggle.\n",
    "\n",
    "**42.966% of tweets are of real disasters, while 57.034% of tweets are not from real disasters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.966"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(100*(train['target'].sum())/train.shape[0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.034"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100-round(100*(train['target'].sum())/train.shape[0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B Split training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = train.sample(frac = .7)\n",
    "dev_set = df = train.merge(training_set, how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1C Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1527</td>\n",
       "      <td>2209</td>\n",
       "      <td>chemical%20emergency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emergency Response and Hazardous Chemical Mana...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>790</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Canada</td>\n",
       "      <td>What a feat! Watch the #BTS of @kallemattson's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4819</td>\n",
       "      <td>6860</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>RT owenrbroadhurst RT JuanMThompson: At this h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6446</td>\n",
       "      <td>9221</td>\n",
       "      <td>suicide%20bombing</td>\n",
       "      <td>GCC</td>\n",
       "      <td>Alleged driver in #Kuwait attack 'joined Daesh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>4124</td>\n",
       "      <td>drought</td>\n",
       "      <td>Las Cruces, NM</td>\n",
       "      <td>Pretty neat website to get the latest drought ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               keyword        location  \\\n",
       "1527  2209  chemical%20emergency             NaN   \n",
       "543    790             avalanche          Canada   \n",
       "4819  6860         mass%20murder     Los Angeles   \n",
       "6446  9221     suicide%20bombing             GCC   \n",
       "2870  4124               drought  Las Cruces, NM   \n",
       "\n",
       "                                                   text  target  \n",
       "1527  Emergency Response and Hazardous Chemical Mana...       0  \n",
       "543   What a feat! Watch the #BTS of @kallemattson's...       0  \n",
       "4819  RT owenrbroadhurst RT JuanMThompson: At this h...       1  \n",
       "6446  Alleged driver in #Kuwait attack 'joined Daesh...       1  \n",
       "2870  Pretty neat website to get the latest drought ...       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first drop the ID column because the label of the tweet will likely not have anything to do with whether or not the tweet is related to a real disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.drop(['id'], inplace=True, axis=1)\n",
    "dev_set.drop(['id', '_merge'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1527</td>\n",
       "      <td>chemical%20emergency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emergency Response and Hazardous Chemical Mana...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Canada</td>\n",
       "      <td>What a feat! Watch the #BTS of @kallemattson's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4819</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>RT owenrbroadhurst RT JuanMThompson: At this h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6446</td>\n",
       "      <td>suicide%20bombing</td>\n",
       "      <td>GCC</td>\n",
       "      <td>Alleged driver in #Kuwait attack 'joined Daesh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>drought</td>\n",
       "      <td>Las Cruces, NM</td>\n",
       "      <td>Pretty neat website to get the latest drought ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   keyword        location  \\\n",
       "1527  chemical%20emergency             NaN   \n",
       "543              avalanche          Canada   \n",
       "4819         mass%20murder     Los Angeles   \n",
       "6446     suicide%20bombing             GCC   \n",
       "2870               drought  Las Cruces, NM   \n",
       "\n",
       "                                                   text  target  \n",
       "1527  Emergency Response and Hazardous Chemical Mana...       0  \n",
       "543   What a feat! Watch the #BTS of @kallemattson's...       0  \n",
       "4819  RT owenrbroadhurst RT JuanMThompson: At this h...       1  \n",
       "6446  Alleged driver in #Kuwait attack 'joined Daesh...       1  \n",
       "2870  Pretty neat website to get the latest drought ...       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make all text lowercase and strip punctuation in order for real disaster tweet predictions to rely only on the content and word sequences in my dataset, as opposed to formatting differences (upper vs. lower case). My data will all be uniform in order to eliminate the possiblity that text formatting influences my classification of tweets.\n",
    "\n",
    "I also get rid of all Twitter usernames, hyperlinks, and random other non-normal English alphabet or Phonecian integers (for example, there's a weird U character that didn't fit into any weird. I remove it below). These text attributes are additional information from the true content of the tweet and therefore I don't anticipate that they will have a major influence on the accurate classification of tweets as real or not real disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    for col in ['keyword', 'location', 'text']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        #get rid of twitter usernames\n",
    "        df[col] = df[col].str.replace('@([\\w]+)','')\n",
    "        #get rid of hyperlinks\n",
    "        df[col] = df[col].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','')\n",
    "        #get rid of all punctuation\n",
    "        df[col] = df[col].str.replace('[^\\w\\s]','')\n",
    "        #weird letter type U in some responses; get rid of it\n",
    "        df[col] = df[col].str.replace('û_', '')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = clean_df(training_set)\n",
    "dev_set = clean_df(dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also get rid of stop words and lemmatize all of the words in order to extract only the unique words that have important meaning in each tweet. I anticipate that these unique words or range of specific words will contribute to accurate classification of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(df):\n",
    "    \n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    df['text'] = df['text'].str.replace(pat, '')\n",
    "    df['text'] = df['text'].str.replace(r'\\s+', ' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = stopwords(training_set)\n",
    "dev_set = stopwords(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1527</td>\n",
       "      <td>chemical20emergency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>emergency response hazardous chemical manageme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>canada</td>\n",
       "      <td>feat watch bts incredible music video avalanche</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4819</td>\n",
       "      <td>mass20murder</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>rt owenrbroadhurst rt juanmthompson hour 70 yr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6446</td>\n",
       "      <td>suicide20bombing</td>\n",
       "      <td>gcc</td>\n",
       "      <td>alleged driver kuwait attack joined daesh day ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>drought</td>\n",
       "      <td>las cruces nm</td>\n",
       "      <td>pretty neat website get latest drought conditi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keyword       location  \\\n",
       "1527  chemical20emergency            NaN   \n",
       "543             avalanche         canada   \n",
       "4819         mass20murder    los angeles   \n",
       "6446     suicide20bombing            gcc   \n",
       "2870              drought  las cruces nm   \n",
       "\n",
       "                                                   text  target  \n",
       "1527  emergency response hazardous chemical manageme...       0  \n",
       "543    feat watch bts incredible music video avalanche        0  \n",
       "4819  rt owenrbroadhurst rt juanmthompson hour 70 yr...       1  \n",
       "6446  alleged driver kuwait attack joined daesh day ...       1  \n",
       "2870  pretty neat website get latest drought conditi...       1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I lemmatize all of the words in order to keep tweets consistent in order for comparison soon (to try to achieve an accurate model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "def lemmatize(df):\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    new = []\n",
    "    for x in df['text']:\n",
    "        y = nltk.word_tokenize(x)\n",
    "        store = []\n",
    "        for word in y:\n",
    "            store.append(wnl.lemmatize(word))\n",
    "        new.append(' '.join(store))\n",
    "\n",
    "    df['text'] = new\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = lemmatize(training_set)\n",
    "dev_set = lemmatize(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1527</td>\n",
       "      <td>chemical20emergency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>emergency response hazardous chemical manageme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>canada</td>\n",
       "      <td>feat watch bts incredible music video avalanche</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4819</td>\n",
       "      <td>mass20murder</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>rt owenrbroadhurst rt juanmthompson hour 70 yr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6446</td>\n",
       "      <td>suicide20bombing</td>\n",
       "      <td>gcc</td>\n",
       "      <td>alleged driver kuwait attack joined daesh day ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>drought</td>\n",
       "      <td>las cruces nm</td>\n",
       "      <td>pretty neat website get latest drought conditi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keyword       location  \\\n",
       "1527  chemical20emergency            NaN   \n",
       "543             avalanche         canada   \n",
       "4819         mass20murder    los angeles   \n",
       "6446     suicide20bombing            gcc   \n",
       "2870              drought  las cruces nm   \n",
       "\n",
       "                                                   text  target  \n",
       "1527  emergency response hazardous chemical manageme...       0  \n",
       "543     feat watch bts incredible music video avalanche       0  \n",
       "4819  rt owenrbroadhurst rt juanmthompson hour 70 yr...       1  \n",
       "6446  alleged driver kuwait attack joined daesh day ...       1  \n",
       "2870  pretty neat website get latest drought conditi...       1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/biaslyai/beginners-guide-to-text-preprocessing-in-python-2cbeafbf5f44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Bag of Words\n",
    "\n",
    "I chose 10 as the minimum number of different tweets words have to appear in order to be included in my model. After calculated all F1 scores when using min_dfs equal in the range of 10 to the entire length of the training set, 10 resulted in the highest F1 score for both training and development sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize the training set\n",
    "count_vect = CountVectorizer(binary=True, min_df=10)\n",
    "X_train = count_vect.fit_transform(training_set['text'])\n",
    "X_test = count_vect.transform(dev_set['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1E Logistic Regression: No Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the same solver parameter for all 3 classifiers below to try to maintain consistent manipulation in order to accurately compare each of their performance. 'Saga' solver was the only solver that works with both no regularization, L1 reg, and L2 reg in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristenflaherty/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(verbose=True, penalty='none', solver='saga')\n",
    "model_noreg = logreg.fit(X_train, training_set['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train = model_noreg.predict(X_train)\n",
    "predicted_dev = model_noreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training: 0.8501362397820164\n",
      "F1 score of development: 0.7194994786235662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_train_noreg = f1_score(training_set['target'], predicted_train)\n",
    "f1_dev_noreg = f1_score(dev_set['target'], predicted_dev)\n",
    "\n",
    "print('F1 score of training:', f1_train_noreg)\n",
    "print('F1 score of development:', f1_dev_noreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With current Bag of Words model, I slightly overfit my data as seen in the higher F1 score of my predictions using my training set, as opposed to my predictions using my development set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1E Logistic Regression: L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 2 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s finished\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(verbose=True, penalty='l1', solver='saga')\n",
    "model_l1 = logreg.fit(X_train, training_set['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train_l1 = model_l1.predict(X_train)\n",
    "predicted_dev_l1 = model_l1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training: 0.8171992481203008\n",
      "F1 score of development: 0.7241003271537622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_train_l1 = f1_score(training_set['target'], predicted_train_l1)\n",
    "f1_dev_l1 = f1_score(dev_set['target'], predicted_dev_l1)\n",
    "\n",
    "print('F1 score of training:', f1_train_l1)\n",
    "print('F1 score of development:', f1_dev_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1E Logistic Regression: L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 79 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(verbose=True, penalty='l2', solver='saga')\n",
    "model_l2 = logreg.fit(X_train, training_set['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train_l2 = model_l2.predict(X_train)\n",
    "predicted_dev_l2 = model_l2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training: 0.8287910552061495\n",
      "F1 score of development: 0.7305002689618074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_train_l2 = f1_score(training_set['target'], predicted_train_l2)\n",
    "f1_dev_l2 = f1_score(dev_set['target'], predicted_dev_l2)\n",
    "print('F1 score of training:', f1_train_l2)\n",
    "print('F1 score of development:', f1_dev_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of training set using:\n",
      "\tNo Regularization: 0.8501362397820164\n",
      "\tL1 Regularization: 0.8171992481203008\n",
      "\tL2 Regularization: 0.8287910552061495\n",
      "\n",
      "F1 Score of development set using:\n",
      "\tNo Regularization: 0.7194994786235662\n",
      "\tL1 Regularization: 0.7241003271537622\n",
      "\tL2 Regularization: 0.7305002689618074\n"
     ]
    }
   ],
   "source": [
    "print('F1 Score of training set using:')\n",
    "print('\\tNo Regularization:', f1_train_noreg)\n",
    "print('\\tL1 Regularization:', f1_train_l1)\n",
    "print('\\tL2 Regularization:', f1_train_l2)\n",
    "\n",
    "print('\\nF1 Score of development set using:')\n",
    "print('\\tNo Regularization:', f1_dev_noreg)\n",
    "print('\\tL1 Regularization:', f1_dev_l1)\n",
    "print('\\tL2 Regularization:', f1_dev_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation:** Logistic Regression with L2 Regularization showed the best performance on the training and development set. Compared to model performances of the models with no regularization and L1 regularization, the model with L2 regularization decreases overfitting on the training set, and maximized performance on the development set, respectively. Even though the classifier with L1 regularization also increased performance of the testing set, it didn't increase model performance of the testing set as much as the model with L2 regularization, and on top of this, the classifier with L1 regularziation showed decreased performance on the development set. This shows that L2 regularization is the optimal regularization for this specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words_coeffs = {}\n",
    "\n",
    "coeffs = model_l1.coef_.tolist()[0]\n",
    "words = [key for key in count_vect.vocabulary_.keys()]\n",
    "for i, val in enumerate(coeffs):\n",
    "    words_coeffs[words[i]] = val\n",
    "    \n",
    "sorted_dict = {}\n",
    "sorted_keys = sorted(words_coeffs, key=words_coeffs.get)  # [1, 3, 2]\n",
    "\n",
    "for w in sorted_keys:\n",
    "    sorted_dict[w] = words_coeffs[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_neg = list(sorted_dict.keys())[:25]\n",
    "top_words_pos = list(sorted_dict.keys())[::-1][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important words for deciding whether a tweet IS about a real disaster:\n",
      "\n",
      " ['second', 'bioterror', 'wreck', 'outrage', 'costlier', 'men', 'gon', 'london', 'ship', 'give', 'hailstorm', 'severe', 'tsunami', 'tried', 'game', 'shoulder', 'link', 'life', 'may', 'night', 'destruction', 'wont', 'detonate', 'pakistan', 'russia']\n"
     ]
    }
   ],
   "source": [
    "print('The most important words for deciding whether a tweet IS about a real disaster:')\n",
    "print('\\n', top_words_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important words for deciding whether a tweet IS NOT about a real disaster:\n",
      "\n",
      " ['reddit', 'season', 'nigerian', 'wound', 'train', 'like', 'fatality', 'teen', 'bear', 'let', 'horrible', 'hey', 'annihilation', 'course', 'enough', 'lead', 'geller', 'cliff', 'think', 'chile', 'listen', 'move', 'owner', '50', 'well']\n"
     ]
    }
   ],
   "source": [
    "print('The most important words for deciding whether a tweet IS NOT about a real disaster:')\n",
    "print('\\n', top_words_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57137498 0.57137498 0.57137498 ... 0.57137498 0.57137498 0.57137498]\n",
      " [0.42862502 0.42862502 0.42862502 ... 0.42862502 0.42862502 0.42862502]] [0.57140176 0.42859824]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.toarray()\n",
    "n = X_train.shape[0] # size of the dataset\n",
    "d = X_train.shape[1] # number of features in our dataset\n",
    "K = 2 # number of clases\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(K):\n",
    "    X_k = X_train[training_set['target'] == k]\n",
    "    #Laplace smoothing\n",
    "    psis[k] = (X_k.shape[0] + 1)/(float(n)+2)\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "\n",
    "# print out the class proportions\n",
    "print(psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we can implement this in numpy\n",
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "\n",
    "idx, logpyx = nb_predictions(X_train, psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, logpyx = nb_predictions(X_test.toarray(), psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on development set using Naive Bayes: 0.603485172730052\n"
     ]
    }
   ],
   "source": [
    "#accuracy of BNB on development set\n",
    "print('F1 score on development set using Naive Bayes:', f1_score(dev_set['target'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Comparison:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Logistic Regression with L2 regularization performed the best in predicting whether a tweet is of a real disaster or not (looking only at performance on development set): it's F1 score was 0.7396 while Naive Bayes predicted the same outcome at 0.6153 (on development set only). Generative models like Naive Bayes have high explanatory power, can detect outliers, and  aren't as computationally expensive as discriminative models. However they can only be used in supervised machine learning because they learn classifications by learning about all of the details of the datatypes/classes it's given, and often result in poorer perfromance than discriminative models. Discriminative models like logistic regression are good because they require less data to make accurate predictions, and produce generally accurate predictions across different types of problems. However, discriminative models are harder to interpret because they seem like black-boxes, and are usually more computationally expensive than generative models.\n",
    "\n",
    "- Naive Bayes assumes that the position of the words in the tweet doesn't affect whether or not the tweet is of a real disaster or not (Bag of Words methods), that each word in a tweet is independent of the other words in the tweet (which makes Naive Bayes for classification purposes robust to outliers), and that their can be any amount of classes to be predicted. These assumptions are different than the ones made by Logistic Regression. Binary logistic Regression assumes that there only two possible predicted classes, that each word in a tweet might affect whether or not the tweet is of a real disaster or not, and that each word in a tweet is linearly related to the log odds. Naive Bayes is a valid model to use for text classification because it doesn't restrict what types of features are inputted into the model - it can predict classes using specific word features or long strings of words. However, it works the best for text classification if it's condition independence assumption about words in tweets holds, and only if the programmer handles the cold start problem - what to do with new, unseen words that it doesn't have prior knowledge for. Overall Naive Bayes is an okay model to use for text classification because of its robustness, efficient computability, and generalizability, but there are more accurate classifiers out there that make less unrealistic assumptions about the data: like logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://primo.ai/index.php?title=Discriminative_vs._Generative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the code below to determine the best M for the CountVectorizer, I found 10 to be the best min_df that optimized model performance, just like as in the logistic regression classifiers above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #choose M\n",
    "\n",
    "# def choose_M(val):\n",
    "    \n",
    "    \n",
    "#     count_vect = CountVectorizer(binary=True, min_df=val, ngram_range=(1,2))\n",
    "#     X_train = count_vect.fit_transform(training_set['text'])\n",
    "#     X_test = count_vect.transform(dev_set['text'])\n",
    "\n",
    "#     logreg = LogisticRegression(verbose=True, penalty='l1', solver='saga')\n",
    "#     model_l1_ngram = logreg.fit(X_train, training_set['target'])\n",
    "\n",
    "#     predicted_train_l1_ngram = model_l1_ngram.predict(X_train)\n",
    "#     predicted_dev_l1_ngram = model_l1_ngram.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#     f1_train_l1_ngram = f1_score(training_set['target'], predicted_train_l1_ngram)\n",
    "#     f1_dev_l1_ngram = f1_score(dev_set['target'], predicted_dev_l1_ngram)\n",
    "\n",
    "# #     print('F1 score of training - with Ngrams:', f1_train_l1_ngram)\n",
    "# #     print('F1 score of development - with Ngrams:', f1_dev_l1_ngram)\n",
    "    \n",
    "#     return f1_train_l1_ngram, f1_dev_l1_ngram\n",
    "    \n",
    "# final_min_df = 0\n",
    "# dev = 0\n",
    "# training = 0\n",
    "\n",
    "# for num in range(10, training_set.shape[0], 10):\n",
    "#     new_training, new_dev = choose_M(num)\n",
    "#     if new_training > training and new_dev > dev:\n",
    "#         final_min_df = num\n",
    "#     else:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize the training set\n",
    "count_vect = CountVectorizer(binary=True, min_df=10, ngram_range=(1,2))\n",
    "X_train = count_vect.fit_transform(training_set['text'])\n",
    "X_test = count_vect.transform(dev_set['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 1-grams and 2-grams: 1260\n",
      "10 2-grams in CountVectorizer Vocab: ['mass murder', 'suicide bombing', 'take quiz', 'cross body', 'dont know', 'christian attacked', 'attacked muslim', 'muslim temple', 'temple mount', 'mount waving']\n"
     ]
    }
   ],
   "source": [
    "print('Total number of 1-grams and 2-grams:', len(count_vect.vocabulary_))\n",
    "\n",
    "twograms = [grams for grams in count_vect.vocabulary_.keys() if len(grams.split(' ')) == 2]\n",
    "\n",
    "print('10 2-grams in CountVectorizer Vocab:', twograms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 0 seconds\n",
      "F1 score of training - with Ngrams: 0.831114225648213\n",
      "F1 score of development - with Ngrams: 0.730124391563007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristenflaherty/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#Log Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "logreg = LogisticRegression(verbose=True, penalty='l2', solver='saga')\n",
    "model_l2_ngram = logreg.fit(X_train, training_set['target'])\n",
    "\n",
    "predicted_train_l2_ngram = model_l2_ngram.predict(X_train)\n",
    "predicted_dev_l2_ngram = model_l2_ngram.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "f1_train_l2_ngram = f1_score(training_set['target'], predicted_train_l2_ngram)\n",
    "f1_dev_l2_ngram = f1_score(dev_set['target'], predicted_dev_l2_ngram)\n",
    "\n",
    "print('F1 score of training - with Ngrams:', f1_train_l2_ngram)\n",
    "print('F1 score of development - with Ngrams:', f1_dev_l2_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use L2 regularization in this logistic regression classifier because of the high L2 regularized classifier performance in section E above. Compared to the performance on the L2 regularized classifier above using Bag of Words, this L2 regularized classifier using Ngrams performs slightly better: the results of the classifier using Bag of Words was 0.8186 for the training set and 0.7396 for the development set. Now, using the Ngrams method, the classifier overfits the training data slightly more (by .03), and performs slightly better on the development set. Overall the results are comprable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57137498 0.57137498 0.57137498 ... 0.57137498 0.57137498 0.57137498]\n",
      " [0.42862502 0.42862502 0.42862502 ... 0.42862502 0.42862502 0.42862502]] [0.57140176 0.42859824]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.toarray()\n",
    "n = X_train.shape[0] # size of the dataset\n",
    "d = X_train.shape[1] # number of features in our dataset\n",
    "K = 2 # number of clases\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(K):\n",
    "    X_k = X_train[training_set['target'] == k]\n",
    "    #Laplace smoothing\n",
    "    psis[k] = (X_k.shape[0] + 1)/(float(n)+2)\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "\n",
    "# print out the class proportions\n",
    "print(psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we can implement this in numpy\n",
    "def nb_predictions_ngrams(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ngrams, logpyx = nb_predictions_ngrams(X_test.toarray(), psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on development set using Naive Bayes and Ngrams: 0.603485172730052\n"
     ]
    }
   ],
   "source": [
    "#accuracy of BNB on development set\n",
    "print('F1 score on development set using Naive Bayes and Ngrams:', f1_score(dev_set['target'], predictions_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the Naive Bayes classifier performance using Bag of Words (0.6153), the Naive Bayes classifier using Ngrams performs the same (printed above). \n",
    "\n",
    "Both Logistic Regression classifiers and Naive Bayes classifiers perform basically the same, if not better, when using the Ngrams technique instead of the Bag of Words technique to vectorize the training set. This implies that the Ngrams created in the vectorization of the training set have the same or slightly higher predictive power than the vecotrs created using the Bag of Words technique, or that the combination of 2 words slightly more impact on the classification of whether a tweet is of a real disaster or not than using single words (no combinations of words, or 2-grams). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model: N-grams using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7608</td>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7609</td>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7610</td>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7611</td>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7612</td>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['id'], inplace=True)\n",
    "test.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean_df(train)\n",
    "test = clean_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = stopwords(train)\n",
    "test = stopwords(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lemmatize(train)\n",
    "test = lemmatize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane holding bridge collapse nearby...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc5km volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating ebike collided car little...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest home razed northern california wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     keyword location                                               text  \\\n",
       "0        NaN      NaN         deed reason earthquake may allah forgive u   \n",
       "1        NaN      NaN              forest fire near la ronge sask canada   \n",
       "2        NaN      NaN  resident asked shelter place notified officer ...   \n",
       "3        NaN      NaN  13000 people receive wildfire evacuation order...   \n",
       "4        NaN      NaN  got sent photo ruby alaska smoke wildfire pour...   \n",
       "...      ...      ...                                                ...   \n",
       "7608     NaN      NaN  two giant crane holding bridge collapse nearby...   \n",
       "7609     NaN      NaN  control wild fire california even northern par...   \n",
       "7610     NaN      NaN                    m194 0104 utc5km volcano hawaii   \n",
       "7611     NaN      NaN  police investigating ebike collided car little...   \n",
       "7612     NaN      NaN  latest home razed northern california wildfire...   \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "7608       1  \n",
       "7609       1  \n",
       "7610       1  \n",
       "7611       1  \n",
       "7612       1  \n",
       "\n",
       "[7613 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire spot pond goose fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon soudelor kill 28 china taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>earthquake safety los angeles ûò safety fasten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>storm ri worse last hurricane cityamp3others h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>green line derailment chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>meg issue hazardous weather outlook hwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cityofcalgary activated municipal emergency pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     keyword location                                               text\n",
       "0        NaN      NaN                        happened terrible car crash\n",
       "1        NaN      NaN  heard earthquake different city stay safe ever...\n",
       "2        NaN      NaN  forest fire spot pond goose fleeing across str...\n",
       "3        NaN      NaN               apocalypse lighting spokane wildfire\n",
       "4        NaN      NaN              typhoon soudelor kill 28 china taiwan\n",
       "...      ...      ...                                                ...\n",
       "3258     NaN      NaN  earthquake safety los angeles ûò safety fasten...\n",
       "3259     NaN      NaN  storm ri worse last hurricane cityamp3others h...\n",
       "3260     NaN      NaN                      green line derailment chicago\n",
       "3261     NaN      NaN            meg issue hazardous weather outlook hwo\n",
       "3262     NaN      NaN  cityofcalgary activated municipal emergency pl...\n",
       "\n",
       "[3263 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize the training set\n",
    "count_vect = CountVectorizer(binary=True, min_df=10, ngram_range=(1,2))\n",
    "X_train = count_vect.fit_transform(train['text'])\n",
    "X_test = count_vect.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 99 epochs took 1 seconds\n",
      "F1 score of training - with Ngrams: 0.8297042573935652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "#Log Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "logreg = LogisticRegression(verbose=True, penalty='l2', solver='saga')\n",
    "model_l2_ngram = logreg.fit(X_train, train['target'])\n",
    "\n",
    "predicted_train_l2_ngram = model_l2_ngram.predict(X_train)\n",
    "predicted_test_l2_ngram = model_l2_ngram.predict(X_test)\n",
    "\n",
    "\n",
    "f1_train_l2_ngram = f1_score(train['target'], predicted_train_l2_ngram)\n",
    "\n",
    "print('F1 score of training - with Ngrams:', f1_train_l2_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.DataFrame(data={'id': ids, 'target': predicted_test_l2_ngram})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle F1 score= 0.78516\n"
     ]
    }
   ],
   "source": [
    "print('Kaggle F1 score=', 0.78516)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This F1 score was about what I expected, except performed better than anticipated. I assumed that my model might have been overfitting my training data, therefore it would have performed badly when exposed to the unseen test data. However, the combination of n-grams and logistic regression resulted in a high performance score. \n",
    "\n",
    "Despite my initial prediction that my model would perform badly on unseen data, it performed better than all of my other models in the past sections. This is probably because the assumption in logistic regression that features are dependent holds in the case of our test data: the presence of each word in a tweet is related to the presence of other words in the tweets, OR it may be the case that our dataset is small enough that if any independencies between the presence of words in tweets were to exist, our sample would be too small for those independencies to severly impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
